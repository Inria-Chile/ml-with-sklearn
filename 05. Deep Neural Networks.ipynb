{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='left' style=\"width:38%;overflow:hidden;\">\n",
    "<a href='http://inria.fr'>\n",
    "<img src='https://github.com/lmarti/jupyter_custom/raw/master/imgs/inr_logo_rouge.png' alt='Inria logo' title='Inria'/>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "align_type": "Left",
     "slide_type": "-"
    }
   },
   "source": [
    "# Machine Learning with `scikit-learn`\n",
    "\n",
    "# Deep Neural Networks\n",
    "\n",
    "## by [Nayat Sánchez Pi](http://www.nayatsanchezpi.com) and [Luis Martí](http://lmarti.com)\n",
    "\n",
    "$\\renewcommand{\\vec}[1]{\\boldsymbol{#1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.rc('font', family='serif')\n",
    "\n",
    "# numpy - pretty matrix \n",
    "np.set_printoptions(precision=3, threshold=1000, edgeitems=5, linewidth=80, suppress=True)\n",
    "\n",
    "import seaborn\n",
    "seaborn.set(style='whitegrid'); seaborn.set_context('talk')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# tikzmagic extesion for figures - https://github.com/mkrphys/ipython-tikzmagic\n",
    "%load_ext tikzmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# fixing a seed for reproducibility, do not do this in real life. \n",
    "random.seed(a=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### About the notebook/slides\n",
    "\n",
    "* The slides are _programmed_ as a [Jupyter](http://jupyter.org)/[IPython](https://ipython.org/) notebook.\n",
    "* **Feel free to try them and experiment on your own by launching the notebooks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If you are using [nbviewer](http://nbviewer.jupyter.org) you can change to slides mode by clicking on the icon:\n",
    "\n",
    "<div class=\"container-fluid\">\n",
    "  <div class=\"row\">\n",
    "      <div class=\"col-md-3\"><span/></div>\n",
    "      <div class=\"col-md-6\">\n",
    "              <img alt='view as slides' src='https://github.com/lmarti/jupyter_custom/raw/master/imgs/view-as-slides.png'/>\n",
    "      </div>\n",
    "      <div class=\"col-md-3\" align='center'><span/></div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Big data\n",
    "\n",
    "* *Buzzword* implying that you are able to handle large amounts of data.\n",
    "* Algorithmical and technical challenges.\n",
    "* Dimensions:\n",
    "    * Number of records or entries (this is mostly a technical challenge).\n",
    "    * Number of variables to take into account.\n",
    "    * Complex data representation i.e. images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Handling images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"container-fluid\">\n",
    "  <div class=\"row\">\n",
    "      <div class=\"col-md-1\" align='center'>\n",
    "      </div>\n",
    "      <div class=\"col-md-10\">\n",
    "              <img src='imgs/06/challenges.jpeg'/>\n",
    "      </div>\n",
    "      <div class=\"col-md-1\" align='center'>\n",
    "      </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dealing with this complex and high-dimensional data\n",
    "\n",
    "* Intuitively we'd expect networks with many more hidden layers to be more powerful.\n",
    "* Such networks could use the intermediate layers to build up multiple levels of abstraction.\n",
    "* Extract progressively more abstract features.\n",
    "* A deep network could have a better performance than a shallow one with the same number of neurons?\n",
    "\n",
    "Deep neural networks at last!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, doing visual pattern recognition:\n",
    "* neurons in the first hidden layer might learn to recognize edges,\n",
    "* neurons in the second layer could learn to recognize more complex shapes, say triangle or rectangles, built up from edges. \n",
    "* The third layer would then recognize still more complex shapes. \n",
    "* And so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"container-fluid\">\n",
    "  <div class=\"row\">\n",
    "      <div class=\"col-md-3\" align='center'>\n",
    "      </div>\n",
    "      <div class=\"col-md-6\">\n",
    "          <img src='imgs/06/AI_system_parts.png'/>\n",
    "          <small> From [Theoretical Motivations for Deep Learning](http://rinuboney.github.io/2015/10/18/theoretical-motivations-deep-learning.html)</small>\n",
    "      </div>\n",
    "      <div class=\"col-md-3\" align='center'>\n",
    "      </div>\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning deep MLP\n",
    "\n",
    "* Intuitively, nothing stop us from training deep MLPs with the backpropagation algorithm.\n",
    "* However, results are worst than *shallow* architectures.\n",
    "\n",
    "### Why?\n",
    "\n",
    "* This contradicts our intuition.\n",
    "* In the worst case we should have layers \"doing nothing\" but not worsening the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Investigating the issue\n",
    "\n",
    "* We need a measure of of progress of learning -> our gradients.\n",
    "* I have already told you that is important to check the gradients.\n",
    "* We have the vectors $\\vec{\\delta}^1,\\ldots,\\vec{\\delta}^l,\\ldots$ of the deltas corresponding to each layer.\n",
    "* We can use the norm of the vector $\\left|\\vec{\\delta}^l\\right|$ as an indicator of how much learning is taking place in each layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gradient descent with just 1,000 training images, trained over 500 epochs on the MNIST dataset.\n",
    "\n",
    "<div class=\"container-fluid\">\n",
    "  <div class=\"row\">\n",
    "      <div class=\"col-md-3\" align='center'>\n",
    "      </div>\n",
    "      <div class=\"col-md-6\">\n",
    "              <img src='imgs/06/training_speed_2_layers.png'/>\n",
    "      </div>\n",
    "      <div class=\"col-md-3\" align='center'>\n",
    "      </div>\n",
    "  </div>\n",
    "</div>\n",
    "<small>More examples on http://neuralnetworksanddeeplearning.com/chap5.html.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The two layers start out learning at very different speeds\n",
    "* The speed in both layers then drops very quickly, before rebounding. \n",
    "* *The first hidden layer learns much more slowly than the second hidden layer*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What about a three hidden layers network?\n",
    "\n",
    "<div class=\"container-fluid\">\n",
    "  <div class=\"row\">\n",
    "      <div class=\"col-md-3\" align='center'>\n",
    "      </div>\n",
    "      <div class=\"col-md-6\">\n",
    "              <img src='imgs/06/training_speed_3_layers.png'/>\n",
    "      </div>\n",
    "      <div class=\"col-md-3\" align='center'>\n",
    "      </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## And with four?\n",
    "\n",
    "<div class=\"container-fluid\">\n",
    "  <div class=\"row\">\n",
    "      <div class=\"col-md-3\" align='center'>\n",
    "      </div>\n",
    "      <div class=\"col-md-6\">\n",
    "              <img src='imgs/06/training_speed_4_layers.png'/>\n",
    "      </div>\n",
    "      <div class=\"col-md-3\" align='center'>\n",
    "      </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The phenomenon is known as the \n",
    "# Vanishing/Exploding Gradient Problem\n",
    "\n",
    "Was reported by:\n",
    "* Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, by Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber (2001).\n",
    "* Sepp Hochreiter's earlier Diploma Thesis, Untersuchungen zu dynamischen neuronalen Netzen (1991, in German).\n",
    "\n",
    "But, probably every body that worked in neural networks had eventually made an experiment like the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* It turns out that the gradient in deep neural networks is **unstable**, \n",
    "* tending to either **explode** or **vanish** in earlier layers. \n",
    "* This instability is a fundamental problem for gradient-based learning in deep neural networks. \n",
    "* It's something we need to understand and address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What's causing the vanishing gradient problem?\n",
    "\n",
    "Lets picture a simple deep network:\n",
    "\n",
    "<div class=\"container-fluid\">\n",
    "  <div class=\"row\">\n",
    "      <div class=\"col-md-2\" align='center'>\n",
    "      </div>\n",
    "      <div class=\"col-md-8\">\n",
    "      <div class='well well-sm'>\n",
    "              <img src='imgs/06/tikz37.png'/>\n",
    "      </div>\n",
    "      </div>\n",
    "      <div class=\"col-md-2\" align='center'>\n",
    "      </div>\n",
    "  </div>\n",
    "</div>\n",
    "Here, $w_1, w_2,\\ldots$, are the weights, $b_1,b_2,\\ldots$ are the biases, and $C$ is some cost function. \n",
    "$$\n",
    "\\frac{\\partial C}{\\partial b_1} = \n",
    "f'(\\text{net}_1)\\times w_2 \\times f'(\\text{net}_2) \\times w_3 \\times f'(\\text{net}_3) \\times w_4 \\times f'(\\text{net}_5) \\times\n",
    "\\frac{\\partial C}{\\partial \\hat{y}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\"What makes deep networks hard to train?\" is a complex question:\n",
    "*  instabilities associated to gradient-based learning in deep networks.\n",
    "* Evidence suggest that there is also a role played by the choice of activation function, \n",
    "* the way weights are initialized, and \n",
    "* how learning by gradient descent is implemented. \n",
    "\n",
    "Many factors play a role in making deep networks hard to train, and understanding all those factors is still a subject of ongoing research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What of we train each layer separately?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## Autoencoders\n",
    "\n",
    "* An autoencoder is typically a MLP neural network which aims to learn a compressed, distributed representation (encoding) of a dataset.\n",
    "* They learn to predict their inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"container-fluid\">\n",
    "  <div class=\"row\">\n",
    "      <div class=\"col-md-2\"><span/></div>\n",
    "      <div class=\"col-md-8\">\n",
    "              <img class='img-thumbnail' alt='Perceptron' src='imgs/autoencoder.png'/>\n",
    "      </div>\n",
    "      <div class=\"col-md-2\" align='center'><span/></div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stacked autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div class=\"container-fluid\">\n",
    "  <div class=\"row\">\n",
    "      <div class=\"col-md-2\"><span/></div>\n",
    "      <div class=\"col-md-8\">\n",
    "              <img class='img-thumbnail' alt='Perceptron' src='imgs/stacked-autoencoder.png'/>\n",
    "      </div>\n",
    "      <div class=\"col-md-2\" align='center'><span/></div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "* The hidden layer of autoencoder $t$  acts as an input layer to autoencoder $t+1$.\n",
    "* The input layer of the first autoencoder is the input layer for the whole network.\n",
    "\n",
    "The greedy layer-wise training procedure works like this:\n",
    "* Train the hidden layers as autoencoders in succession.\n",
    "* Train final (output layer) to predict targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Can we look back (again) to nature for inspiration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks (CNNs / ConvNets)\n",
    "\n",
    "Convolutional Neural Networks are very similar to ordinary neural networks:\n",
    "\n",
    "* They are made up of neurons that have learnable weights and biases. \n",
    "* Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linear activation. \n",
    "* The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other.\n",
    "* They still have a loss function on the last (fully-connected) layer and all the tips/tricks we debated for learning regular NN still apply.\n",
    "\n",
    "Difference: \n",
    "* ConvNets make the explicit assumption that the inputs are images, this allows to encode certain properties into the architecture. \n",
    "* Forward function more efficient to implement and vastly reduce the amount of parameters in the network.\n",
    "* Main application: image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Yet another problem with regular NNs\n",
    "\n",
    "* Regular Neural Nets don’t scale well to full images.\n",
    "* Images  only of size $32\\times32\\times3$ (32 wide, 32 high, 3 color channels), imply that the first hidden layer will have $32\\times 32\\times3 = 3072$ weights.\n",
    "* This fully-connected structure does not scale to larger images. \n",
    "* For example, an image of more \"normal\" size, e.g. $200\\times 200\\times3$, would lead to neurons that have  $200\\times 200\\times3$ = $120,000$ weights!\n",
    "* Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! \n",
    "* Clearly, this full connectivity is wasteful.\n",
    "* The huge number of parameters would quickly lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Convolutional Neural Networks take advantage of the fact that the input consists of images\n",
    "\n",
    "* constrain the architecture in a more sensible way. \n",
    "* unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth.\n",
    "* neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner.\n",
    "* The ConvNet architecture we will reduce the full image into a single vector of class scores, arranged along the depth dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"container-fluid\">\n",
    "  <div class=\"row\">\n",
    "      <div class=\"col-md-5\">\n",
    "      Regular network <img src='imgs/06/conv.jpeg'/>\n",
    "      <div class=\"col-md-2\" align='center'>\n",
    "          &nbsp;\n",
    "      </div>\n",
    "      </div>\n",
    "      <div class=\"col-md-5\">\n",
    "      ConvNet <img src='imgs/06/cnn.jpeg'/>\n",
    "      </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key concepts\n",
    "\n",
    "* **Local receptive fields**: We won't connect every input pixel to every hidden neuron. Instead, we only make connections in small, localized regions of the input image.\n",
    "* **Shared weights and biases**.\n",
    "* **Pooling**: usually used immediately after convolutional layers. They simplify the information in the output from the convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ConvNets layer types\n",
    "\n",
    "* As we described above, a ConvNet is a sequence of layers, and\n",
    "* every layer of a ConvNet transforms one volume of activations to another through a differentiable function. \n",
    "* Three main types of layers to build ConvNet architectures: \n",
    "    * Convolutional Layer, \n",
    "    * Rectified linear units (RELU) Layer,\n",
    "    * Pooling Layer, and \n",
    "    * Fully-Connected Layer (exactly as seen in regular Neural Networks). \n",
    "\n",
    "We will stack these layers to form a full ConvNet architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"container-fluid\">\n",
    "  <div class=\"row\">\n",
    "      <div class=\"col-md-1\" align='center'>\n",
    "      </div>\n",
    "      <div class=\"col-md-10\">\n",
    "      <div class='well well-sm' align='center'>\n",
    "              <img src='imgs/06/convnet.jpeg'/>\n",
    "      </div>\n",
    "      </div>\n",
    "      <div class=\"col-md-1\" align='center'>\n",
    "      </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Layer\n",
    "\n",
    "The convolutional layer is the core building block of a convolutional network that does most of the computational heavy lifting.\n",
    "* consist of a set of learnable filters. \n",
    "* Every filter is small spatially (along width and height), but extends through the full depth of the input volume. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example,\n",
    "* a typical filter on a first layer of a ConvNet might have size 5x5x3 (i.e. 5 pixels width and height, and 3 color channels).\n",
    "* During the forward pass, we slide (*convolve*) each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position. \n",
    "* We will produce a 2-dimensional activation map that gives the responses of that filter at every spatial position.\n",
    "\n",
    "![Convolution example](http://ufldl.stanford.edu/tutorial/images/Convolution_schematic.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The network will learn filters that activate when they see some type of visual feature:\n",
    "    * an edge of some orientation or a blotch of some color on the first layer, or \n",
    "    * entire honeycomb or wheel-like patterns on higher layers of the network.\n",
    "* We will have an entire set of filters in each CONV layer (e.g. 12 filters), and each of them will produce a separate 2-dimensional activation map. We will stack these activation maps along the depth dimension and produce the output volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spatial arrangement\n",
    "\n",
    "How many neurons there are in the output volume and how they are arranged?\n",
    "Three hyperparameters control the size of the output volume: \n",
    "* depth, \n",
    "* stride and \n",
    "* zero-padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Depth of the output volume \n",
    "\n",
    "Corresponds to the number of filters we would like to use, \n",
    "* each learning to look for something different in the input. \n",
    "\n",
    "For example, if the first Convolutional Layer takes as input the raw image, then different neurons along the depth dimension may activate in presence of various oriented edged, or blobs of color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stride\n",
    "\n",
    "* We must specify the stride with which we slide the filter. \n",
    "* When the stride is 1 then we move the filters one pixel at a time.\n",
    "* When the stride is 2 (or uncommonly 3 or more, though this is rare in practice) then the filters jump 2 pixels at a time as we slide them around. \n",
    "* This will produce smaller output volumes spatially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Zero padding\n",
    "\n",
    "* Sometimes it is convenient to pad the input volume with zeros around the border.\n",
    "* The nice feature of zero padding is that it will allow us to control the spatial size of the output volumes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Shared weight and biases\n",
    "\n",
    "* All neurons in the layer detect the same *feature*.\n",
    "* We need to add layers to encode more features.\n",
    "\n",
    "<div align='center'>\n",
    "<img src='http://neuralnetworksanddeeplearning.com/images/tikz46.png' width='47%'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"container-fluid\">\n",
    "  <div class=\"row\">\n",
    "      <div class=\"col-md-1\" align='center'>\n",
    "      </div>\n",
    "      <div class=\"col-md-10\">\n",
    "      <div class='well well-sm' align='center'>\n",
    "              <img src='imgs/06/weights.jpeg'/>\n",
    "      </div>\n",
    "      </div>\n",
    "      <div class=\"col-md-1\" align='center'>\n",
    "      </div>\n",
    "  </div>\n",
    "</div>\n",
    "Krizhevsky et al. Each of the 96 filters shown here is of size $[11\\times11\\times3]$, and each one is shared by the $55\\times55$ neurons in one depth slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing the Convolutional layer\n",
    "* Accepts a volume of size $W_1\\times H_1\\times D_1$\n",
    "* Requires four hyperparameters:\n",
    "    * Number of filters $K$,\n",
    "    * their spatial extent $F$,\n",
    "    * the stride $S$,\n",
    "    * the amount of zero padding $P$.\n",
    "Produces a volume of size $W_2\\times H_2\\times D_2$ where:\n",
    "$$\n",
    "W_2 = \\frac{W_1−F+2P}{S+1},\\ H_2 = \\frac{H_1−F+2P}{S+1},\\ D_2=K\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling\n",
    "\n",
    "* Subsampling layers reduce the size of the input. \n",
    "\n",
    "* There are multiple ways to subsample, but the most popular are:\n",
    "    - max pooling (most popular), \n",
    "    - average pooling, and\n",
    "    - stochastic pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In max-pooling, a pooling unit simply outputs the maximum activation in the 2×22×2 input region:\n",
    "<div class=\"container-fluid\">\n",
    "  <div class=\"row\">\n",
    "      <div class=\"col-md-1\" align='center'>\n",
    "      </div>\n",
    "      <div class=\"col-md-10\">\n",
    "      <div class='well well-sm' align='center'>\n",
    "              <img src='http://neuralnetworksanddeeplearning.com/images/tikz47.png'/>\n",
    "      </div>\n",
    "      </div>\n",
    "      <div class=\"col-md-1\" align='center'>\n",
    "      </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can see convolution as the application of a filter or a dimensionality reduction.\n",
    "\n",
    "* Convolutional layers apply a number of filters to the input. \n",
    "* The result of one filter applied across the image is called feature map.\n",
    "* If the previous layer is also convolutional, the filters are applied across all of it’s FMs with different weights, so each input FM is connected to each output FM. \n",
    "> The intuition behind the shared weights across the image is that the features will be detected regardless of their location, while the multiplicity of filters allows each of them to detect different set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The convolutional architecture is quite different to the architecture of traditional neural network.\n",
    "* But the overall picture is similar: \n",
    "    * a network made of many simple units, \n",
    "    * whose behaviors are determined by their weights and biases. \n",
    "The overall goal is still the same: to use training data to train the network's weights and biases so that the network does a good job classifying input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Restricted Boltzmann machines\n",
    "\n",
    "Restricted Boltzmann machines (RBM) are generative stochastic neural network that can learn a probability distribution over its set of inputs.\n",
    "\n",
    "<div class=\"container-fluid\">\n",
    "  <div class=\"row\">\n",
    "      <div class=\"col-md-3\" align='center'>\n",
    "      </div>\n",
    "      <div class=\"col-md-6\">\n",
    "      <div class='well well-sm' align='center'>\n",
    "              <img src='imgs/06/rbm.png'/>\n",
    "      </div>\n",
    "      </div>\n",
    "      <div class=\"col-md-3\" align='center'>\n",
    "      </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training RBMs: Contrastive Divergence\n",
    "\n",
    "* **Positive phase:**\n",
    "    - An input sample $\\vec{x}$ is presented to the input layer. \n",
    "    - $\\vec{x}$ is propagated to the hidden layer in a similar manner to the feedforward networks. \n",
    "    - The result is the hidden layer activations, $\\vec{h}$.\n",
    "* **Negative phase:**\n",
    "    - Propagate $\\vec{h}$ back to the visible layer with result resulting in a $\\vec{x}'$.\n",
    "    - $\\vec{x}'$ back to the hidden layer.\n",
    "* **Weight update**:\n",
    "$$ \\vec{w}(t+1) = \\vec{w}(t) + \\alpha (\\vec{x}\\vec{h}^{\\intercal} -\\vec{x'}\\vec{h'}^{\\intercal}).$$\n",
    "\n",
    "* The positive phase reflects the network internal representation of the data.\n",
    "* The negative phase represents an attempt to recreate the data based on this internal representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The goal is that the \"generated\" data to be as close as possible to the \"real\" one.\n",
    "* This is reflected in the weight update formula.\n",
    "\n",
    "> In other words, the net has a perception of how the input data must be represented, so it tries to reproduce the data based on this perception. If its reproduction isn’t close enough to reality, it makes an adjustment and tries again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Belief Networks\n",
    "\n",
    "Restricted Boltzmann machines can be stacked to create a class of neural networks known as deep belief networks (DBNs).\n",
    "<div class=\"container-fluid\">\n",
    "  <div class=\"row\">\n",
    "      <div class=\"col-md-3\" align='center'>\n",
    "      </div>\n",
    "      <div class=\"col-md-6\">\n",
    "      <div class='well well-sm' align='center'>\n",
    "              <img src='imgs/06/deep-belief.png'/>\n",
    "      </div>\n",
    "      </div>\n",
    "      <div class=\"col-md-3\" align='center'>\n",
    "      </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Programming Deep Learning\n",
    "\n",
    "* Torch - An open source software library for machine learning based on the Lua programming language.\n",
    "* Caffe - A deep learning framework.\n",
    "* Apache SINGA - A deep learning platform developed for scalability, usability and extensibility.\n",
    "\n",
    "My current preference:\n",
    "* Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# General Principles\n",
    "\n",
    "* Supervise the learning process (did I mentioned that you should check your gradients?).\n",
    "* Use a correct experimental methodology.\n",
    "* Contrast your results with a baseline method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Applications of Deep Learning](https://en.wikipedia.org/wiki/Deep_learning#Applications)\n",
    "\n",
    "There are many successfull applications, for example:\n",
    "\n",
    "* Computer vision and image recognition;\n",
    "* Speech recognition;\n",
    "* Natural language processing $\\rightarrow$ probabilistic context free grammars;\n",
    "* Anomaly detection on many variables;\n",
    "* ... and many more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Final remarks\n",
    "\n",
    "* Deep learning as a step towards realising *strong AI*;\n",
    "* thus many organizations have become interested in its use for particular applications.\n",
    "    * see https://en.wikipedia.org/wiki/Deep_learning#Commercial_activities\n",
    "* Better understanding of mental processes.\n",
    "* Deep learning $\\iff$ big data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<hr/>\n",
    "<div class=\"container-fluid\">\n",
    "      <div class=\"row\">\n",
    "          <div class=\"col-md-3\" align='center'>\n",
    "              <img align='center' alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\"/>\n",
    "          </div>\n",
    "          <div class=\"col-md-9\">\n",
    "              This work is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).\n",
    "          </div>\n",
    "      </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" href=\"https://files.inria.fr/dircom/extranet/fonts-inria-sans.css\" rel=\"stylesheet\">\n",
       "<link rel=\"stylesheet\" href=\"https://files.inria.fr/dircom/extranet/fonts-inria-serif.css\" rel=\"stylesheet\">\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Inconsolata\" rel=\"stylesheet\">\n",
       "<style>\n",
       ".text_cell_render {\n",
       "font-style: regular;\n",
       "font-family: \"Inria Serif\", Georgia, serif; \n",
       "display: block;\n",
       "}\n",
       "/*font-weight: 200;*/\n",
       "/*text-align: left;\n",
       "line-height: 100%;\n",
       "display: block;\n",
       "}*/\n",
       ".text_cell_render h1 {\n",
       "/*font-size: 24pt;*/\n",
       "font-family: 'Inria Sans', \"Helvetica Neue\", Arial, \"Noto Sans\", sans-serif;\n",
       "font-weight: bold;\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.5em;\n",
       "color:#4a4a4a;\n",
       "}\n",
       "\n",
       ".reveal h1 {\n",
       "font-family: 'Inria Sans', \"Helvetica Neue\", Arial, \"Noto Sans\", sans-serif;\n",
       "/*font-size: 24pt;*/\n",
       "font-weight: bold;\n",
       "margin-bottom: 0.47em;\n",
       "margin-top: 0.74em;\n",
       "color:#4a4a4a;\n",
       "}\n",
       ".text_cell_render h2 {\n",
       "/*font-size: 21pt;*/\n",
       "    font-family: 'Inria Sans', \"Helvetica Neue\", Arial, \"Noto Sans\", sans-serif;\n",
       "margin-bottom: 0.29em;\n",
       "margin-top: 0.38em;\n",
       "color:#595959;\n",
       "}\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Inria Sans', \"Helvetica Neue\", Arial, \"Noto Sans\", sans-serif;\n",
       "/*font-size: 19pt;*/\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.3em;\n",
       "color:#595959;\n",
       "}\n",
       ".text_cell_render h4 {\n",
       "    font-family: 'Inria Sans', \"Helvetica Neue\", Arial, \"Noto Sans\", sans-serif;\n",
       "/*font-size: 17pt;*/\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.3em;\n",
       "color:#595959;\n",
       "}\n",
       ".text_cell_render h5 {\n",
       "    font-family: 'Inria Sans', \"Helvetica Neue\", Arial, \"Noto Sans\", sans-serif;\n",
       "/*font-size: 15pt;*/\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.3em;\n",
       "color:#595959;\n",
       "}\n",
       "div.text_cell_render{\n",
       "line-height: 120%;\n",
       "font-size: 100%;\n",
       "font-weight: 400;\n",
       "text-align: justify;\n",
       "margin-left:0em;\n",
       "margin-right:0em;\n",
       "}\n",
       ".reveal div.text_cell_render{\n",
       "line-height: 120%;\n",
       "font-size: 74%;\n",
       "font-weight: 400;\n",
       "text-align: justify;\n",
       "margin-left:0em;\n",
       "margin-right:0em;\n",
       "}\n",
       "\n",
       ".reveal h2 {\n",
       "font-family: 'Inria Sans', \"Helvetica Neue\", Arial, \"Noto Sans\", sans-serif;\n",
       "/*font-size: 24pt;*/\n",
       "font-weight: bold;\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.5em;\n",
       "color:#595959;\n",
       "}\n",
       ".reveal h3 {\n",
       "font-family: 'Inria Sans', \"Helvetica Neue\", Arial, \"Noto Sans\", sans-serif;\n",
       "/*font-size: 24pt;*/\n",
       "font-weight: bold;\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.5em;\n",
       "color:#595959;\n",
       "}\n",
       ".reveal h4 {\n",
       "font-family: 'Inria Sans', \"Helvetica Neue\", Arial, \"Noto Sans\", sans-serif;\n",
       "font-weight: bold;\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.5em;\n",
       "color:#595959;\n",
       "}\n",
       ".reveal .code_cell {\n",
       "    font-size: 92%;\n",
       "}\n",
       ".reveal code {\n",
       "font-family: 'Inconsolata', monospace;\n",
       "}\n",
       ".reveal pre {\n",
       "font-family: 'Inconsolata', monospace;\n",
       "}\n",
       "code {\n",
       "font-family: 'Inconsolata', monospace;\n",
       "}\n",
       "pre {\n",
       "font-family: 'Inconsolata', monospace;\n",
       "}\n",
       ".CodeMirror{\n",
       "font-family: \"Inconsolata\", monospace;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this code is here for cosmetic reasons\n",
    "from IPython.core.display import HTML\n",
    "from urllib.request import urlopen\n",
    "HTML(urlopen('https://raw.githubusercontent.com/lmarti/jupyter_custom/master/custom.include').read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
